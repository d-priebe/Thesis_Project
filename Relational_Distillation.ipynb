{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "from torch.nn import HuberLoss\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac361eb6",
   "metadata": {},
   "source": [
    "# Teacher Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teacher Architecture: Code from Cretois et al. (2022)\n",
    "class VGG11(nn.Module):\n",
    "    def __init__(self, T=5.0):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "\n",
    "        # First set of conv layers -> depth of 64\n",
    "        self.conv11 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn11  = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Second set of conv layers -> from depth 64 to depth 128\n",
    "        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn21  = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Third set of conv layers -> from depth 128 to depth 256\n",
    "        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn31  = nn.BatchNorm2d(256)\n",
    "        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32  = nn.BatchNorm2d(256)\n",
    "                      \n",
    "        # Fourth set of conv layers -> from depth 128 to depth 256\n",
    "        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn41  = nn.BatchNorm2d(512)\n",
    "        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42  = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Fifth set of conv layers -> from depth 128 to depth 256\n",
    "        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn51  = nn.BatchNorm2d(512)\n",
    "        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn52  = nn.BatchNorm2d(512)\n",
    "              \n",
    "        # First FC layer\n",
    "        self.fc1 = nn.Linear(4 * 4 * 512,  4096)\n",
    "        # Second FC layer\n",
    "        self.fc2 = nn.Linear( 4096,  4096)\n",
    "        \n",
    "        # Add a dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Output\n",
    "        self.fc3 = nn.Linear(4096, 1)\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # MaxPool for the first block --> img from 128x128 to 64x64\n",
    "        out = F.max_pool2d(torch.relu(self.bn11(self.conv11(x))), 2)\n",
    "\n",
    "        # MaxPool for the first block --> img from 64x64 to 32x32\n",
    "        out = F.max_pool2d(torch.relu(self.bn21(self.conv21(out))), 2)\n",
    "\n",
    "        # MaxPool for the first block --> img from 32x32 to 16x16\n",
    "        out = torch.relu(self.bn31(self.conv31(out)))\n",
    "        out = F.max_pool2d(torch.relu(self.bn32(self.conv32(out))), 2)\n",
    "        \n",
    "        # MaxPool for the first block --> img from 16x16 to 8x8\n",
    "        out = torch.relu(self.bn41(self.conv41(out)))\n",
    "        out = F.max_pool2d(torch.relu(self.bn42(self.conv42(out))), 2)\n",
    "        \n",
    "        # MaxPool for the first block --> img from 8x8 to 4x4\n",
    "        out = torch.relu(self.bn51(self.conv51(out)))\n",
    "        out = F.max_pool2d(torch.relu(self.bn52(self.conv52(out))), 2)\n",
    "        \n",
    "        # Flatten the whole thing: image of 4 x 4 * 512 \n",
    "        out = out.view(-1, 4 * 4 * 512)\n",
    "        out = self.dropout(torch.relu(self.fc1(out)))\n",
    "        out = self.dropout(torch.relu(self.fc2(out)))\n",
    "        \n",
    "        # Return logits instead of sigmoid output\n",
    "        logits = self.fc3(out)\n",
    "        return logits / self.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610b5c7",
   "metadata": {},
   "source": [
    "# Student Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d47e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efficient Student 1\n",
    "\n",
    "# SE Layer: Code adapted from:\n",
    "#d-li14. (2021). Mobilenetv3.pytorch [Source code]. GitHub. https://github.com/d-li14/mobilenetv3.pytorch/tree/master\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        #Squeeze Operation - Generates vector size of M\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        #Excitation Operation\n",
    "        self.fc = nn.Sequential(\n",
    "            #reduces the dimensionality of the squeezed vector\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #restores the original dimensionality of the squeezed vector \n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            #Apply sigmoid to produce channel-wise scaling factors\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        #Multiply input tensor by channel- wise factor for recalibration of channel wise features\n",
    "        return x * y.expand_as(x)\n",
    "   \n",
    "\n",
    " # Bottleneck Layer: partial-Code adapted from:\n",
    "#d-li14. (2021). Mobilenetv3.pytorch. GitHub. https://github.com/d-li14/mobilenetv3.\n",
    "#Generative language model (GPT 3.5 OpenAI (2023))\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expansion, use_se):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        mid_channels = in_channels * expansion\n",
    "        self.use_se = use_se\n",
    "        self.layers = nn.Sequential(\n",
    "            #Pointwise - Expansion\n",
    "            nn.Conv2d(in_channels, mid_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #Depth-wise Convolution\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size, stride, padding=kernel_size // 2, groups=mid_channels, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #Pointwise - Reduction to desired output\n",
    "            nn.Conv2d(mid_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        if self.use_se:\n",
    "            self.se = SELayer(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        # Apply IRB output to SE block\n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "        out += self.shortcut(x) if self.shortcut else out\n",
    "        return nn.ReLU(inplace=True)(out)\n",
    "\n",
    "#STUDENT\n",
    "class EfficientStudent1(nn.Module):\n",
    "    def __init__(self, num_classes=1, T=5.0):\n",
    "        super(EfficientStudent1, self).__init__()\n",
    "        self.T = T\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 1, 1, bias=False), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(16, 16, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(16, 32, 3, 2, 2, use_se=False),\n",
    "            Bottleneck(32, 32, 3, 1, 4, use_se=False),\n",
    "            Bottleneck(32, 64, 3, 2, 6, use_se=True),\n",
    "            Bottleneck(64, 64, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(64, 64, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(64, 128, 3, 2, 4, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 6, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(128, 256, 3, 2, 4, use_se=True),  \n",
    "            Bottleneck(256, 256, 3, 1, 6, use_se=True),\n",
    "            Bottleneck(256, 512, 3, 2, 2, use_se=True),  \n",
    "            nn.Conv2d(512, 512, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 512, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, num_classes, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x,get_features=False):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientStudent2\n",
    "class EfficientStudent2(nn.Module):\n",
    "    def __init__(self, num_classes=1, T=5.0):\n",
    "        super(EfficientStudent2, self).__init__()\n",
    "        self.T = T\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 1, 1, bias=False), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(16, 16, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(16, 32, 3, 2, 2, use_se=False),\n",
    "            Bottleneck(32, 32, 3, 1, 4, use_se=False),\n",
    "            Bottleneck(32, 64, 3, 2, 6, use_se=True),\n",
    "            Bottleneck(64, 64, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(64, 128, 3, 2, 4, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 6, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(128, 256, 3, 2, 4, use_se=True),  \n",
    "            Bottleneck(256, 256, 3, 1, 6, use_se=True),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1, bias=False),  \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 512, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, num_classes, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3af37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientStudent3\n",
    "\n",
    "\n",
    "class EfficientStudent3(nn.Module):\n",
    "    def __init__(self, num_classes=1, T=5.0):\n",
    "        super(EfficientStudent3, self).__init__()\n",
    "        self.T = T\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, 3, 1, 1, bias=False), \n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(4, 4, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(4, 8, 3, 2, 2, use_se=False),\n",
    "            Bottleneck(8, 16, 3, 2, 4, use_se=False),\n",
    "            Bottleneck(16, 16, 3, 1, 6, use_se=True),\n",
    "            Bottleneck(16, 32, 3, 2, 2, use_se=True),\n",
    "            Bottleneck(32, 32, 3, 1, 4, use_se=True),\n",
    "            Bottleneck(32, 64, 3, 2, 6, use_se=True),\n",
    "            Bottleneck(64, 64, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(64, 128, 3, 2, 4, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 6, use_se=True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(128, 128, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, num_classes, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd23562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientStudent4\n",
    "class EfficientStudent4(nn.Module):\n",
    "    def __init__(self, num_classes=1, T=5.0):\n",
    "        super(EfficientStudent4, self).__init__()\n",
    "        self.T = T\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(4, 4, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(4, 8, 3, 2, 1, use_se=False),\n",
    "            Bottleneck(8, 16, 3, 2, 1, use_se=True),\n",
    "            Bottleneck(16, 32, 3, 2, 1, use_se=True),\n",
    "            Bottleneck(32, 32, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(32, 64, 3, 2, 1, use_se=True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(64, 64, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_classes, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5d1b1",
   "metadata": {},
   "source": [
    "# Feature Extraction, Dataloader, & DataSet functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mel-Spec Generation\n",
    "def generate_mel_spectrogram(x, sr, show=False, resize=True):\n",
    "    sgram = librosa.stft(x, n_fft=1024, hop_length=376)\n",
    "    sgram_mag, _ = librosa.magphase(sgram)\n",
    "    mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sr, n_mels=128)\n",
    "    mel_sgram = librosa.amplitude_to_db(mel_scale_sgram)\n",
    "    if resize:\n",
    "        # Crop the mel spectrogram to 128x128\n",
    "        mel_sgram = mel_sgram[:, :128]\n",
    "    if show:\n",
    "        librosa.display.specshow(mel_sgram, sr=sr, x_axis='time', y_axis='mel')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "    return mel_sgram\n",
    "\n",
    "# Dataset\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        x = torch.tensor(x)\n",
    "        return x, torch.tensor(y).unsqueeze(-1)\n",
    "        \n",
    "    \n",
    "  \n",
    " # Load data\n",
    "def load_data(data_path):\n",
    "    speech_dir = os.path.join(data_path, 'speech')\n",
    "    no_speech_dir = os.path.join(data_path, 'no_speech')\n",
    "\n",
    "    speech_files = [os.path.join(speech_dir, f) for f in os.listdir(speech_dir) if f.endswith('.wav')]\n",
    "    no_speech_files = [os.path.join(no_speech_dir, f) for f in os.listdir(no_speech_dir) if f.endswith('.wav')]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in speech_files + no_speech_files:\n",
    "        x, sr = librosa.load(file)\n",
    "        mel_sgram = generate_mel_spectrogram(x, sr)\n",
    "        data.append(mel_sgram)\n",
    "    \n",
    "    labels = [1] * len(speech_files) + [0] * len(no_speech_files)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6175c1",
   "metadata": {},
   "source": [
    "# Relational distillation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates Angle loss between teacher-student logits\n",
    "def rkd_angle_loss(student_logits, teacher_logits):\n",
    "    student_angles = torch.matmul(student_logits, student_logits.t())\n",
    "    teacher_angles = torch.matmul(teacher_logits, teacher_logits.t())\n",
    "\n",
    "    student_angles = torch.clamp(student_angles, min=-1.0, max=1.0)\n",
    "    teacher_angles = torch.clamp(teacher_angles, min=-1.0, max=1.0)\n",
    "    #Instantiate Huber loss\n",
    "    angle_loss = HuberLoss(delta=1.0)\n",
    "    #Calculate Angle loss as Huber loss\n",
    "    return angle_loss(torch.acos(student_angles), torch.acos(teacher_angles))\n",
    "\n",
    "#Calculates total distillation loss between teacher-student logits\n",
    "def rkd_distillation_loss(student_logits, teacher_logits, alpha, beta):\n",
    "    distance_loss = HuberLoss(delta=1.0)\n",
    "    distance_loss_value = distance_loss(torch.pdist(student_logits), torch.pdist(teacher_logits.detach()))\n",
    "    angle_loss_value = rkd_angle_loss(student_logits, teacher_logits)\n",
    "    # Return the total loss as weighted sum \n",
    "    return alpha * distance_loss_value + beta * angle_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f68852",
   "metadata": {},
   "source": [
    "# Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Block\n",
    "\n",
    "def train_student(teacher, student, train_loader, val_loader, num_epochs, T, alpha, beta,device, patience=3):\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    \n",
    "    # Freeze teacher \n",
    "    for param in teacher.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    #Student-loss and & optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=.001)\n",
    "\n",
    "    # Early stopping setup\n",
    "    best_val_loss = np.inf\n",
    "    no_improvement_count = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "      \n",
    "            # Forward pass through student \n",
    "            student_logits = student(inputs)\n",
    "            \n",
    "            loss_student = criterion(student_logits, labels.float())\n",
    "\n",
    "            # Forward pass through teacher \n",
    "            teacher_logits = teacher(inputs)\n",
    "            teacher_prob = torch.sigmoid(teacher_logits / T)\n",
    "          \n",
    "            # Calculate distillation loss\n",
    "            loss_distill = rkd_distillation_loss(student_logits, teacher_logits, alpha, beta)\n",
    "    \n",
    "            # Combine total loss\n",
    "            loss =  loss_student + alpha * loss_distill\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / (i + 1)}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        student.eval()\n",
    "        val_loss = 0\n",
    "        auc_scores = []\n",
    "        f1_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "\n",
    "                student_logits = student(inputs)\n",
    "                student_prob = torch.sigmoid(student_logits / T)\n",
    "\n",
    "                #Calculate AUC\n",
    "                auc = roc_auc_score(labels.cpu().numpy(), student_prob.cpu().numpy())\n",
    "                auc_scores.append(auc)\n",
    "            \n",
    "                #Calculate F1\n",
    "                predicted = (student_prob > 0.5).squeeze().long().cpu().numpy()\n",
    "                f1 = f1_score(labels.cpu().numpy(), predicted)\n",
    "                f1_scores.append(f1)\n",
    "            \n",
    "                val_loss += criterion(student_logits, labels.float()).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        mean_auc = np.mean(auc_scores)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        print(f'Validation Loss: {val_loss}, AUC: {mean_auc}, F1 Score: {mean_f1}')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f'Early stopping after {epoch + 1} epochs')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99773350",
   "metadata": {},
   "source": [
    "# Test Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5503fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_student(student, test_loader, device):\n",
    "    student.to(device)\n",
    "    student.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "            student_prob = torch.sigmoid(student_logits)\n",
    "\n",
    "            # Calculate AUC\n",
    "            auc = roc_auc_score(labels.cpu().numpy(), student_prob.cpu().numpy())\n",
    "            auc_scores.append(auc)\n",
    "\n",
    "            # Calculate F1\n",
    "            predicted = (student_prob > 0.5).squeeze().long().cpu().numpy()\n",
    "            f1 = f1_score(labels.cpu().numpy(), predicted)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            test_loss += criterion(student_logits, labels.float()).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    print(f'Test Loss: {test_loss}, AUC: {mean_auc}, F1 Score: {mean_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a1dd3",
   "metadata": {},
   "source": [
    "# Training Instantiations - Models, Datasets, Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a70ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Instantiation:\n",
    "Teacher_Weights = '/content/drive/MyDrive/Thesis_Material/ecoVAD_model_weight.pt'\n",
    "Train_Data ='/content/drive/MyDrive/Thesis_Material/Synthetic_Dataset'\n",
    "\n",
    "#Train_Data ='/content/drive/MyDrive/Thesis_Material/Validation_Dataset'\n",
    "Evaluation_Data = '/content/drive/MyDrive/Thesis_Material/playback_data'\n",
    "# Load teacher model and weights\n",
    "\n",
    "teacher = VGG11()\n",
    "teacher.load_state_dict(torch.load(Teacher_Weights))  \n",
    "\n",
    "# student model(s)\n",
    "student1 = EfficientStudent1()\n",
    "student2 = EfficientStudent2()\n",
    "student3 = EfficientStudent3()\n",
    "student4 = EfficientStudent4()\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs = 50\n",
    "T = 5.0\n",
    "alpha = 0.2\n",
    "beta = .3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Load data\n",
    "data, labels = load_data(Train_Data)  \n",
    "\n",
    "# Split data into train, val, test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# SpeechDataset instances for training, validation, testing\n",
    "train_dataset = SpeechDataset(train_data, train_labels, transform=ToTensor())\n",
    "val_dataset = SpeechDataset(val_data, val_labels, transform=ToTensor())\n",
    "test_dataset = SpeechDataset(test_data, test_labels, transform=ToTensor())\n",
    "\n",
    "# DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7158c9",
   "metadata": {},
   "source": [
    "# Train & Test Student Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df16f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train Model:\n",
    "\n",
    "train_student(teacher, student1, train_loader, val_loader, num_epochs, T, alpha,beta, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0589d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_student(student1, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894dc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train Model:\n",
    "\n",
    "train_student(teacher, student2, train_loader, val_loader, num_epochs, T, alpha,beta, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_student(student2, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440de113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model:\n",
    "\n",
    "train_student(teacher, student3, train_loader, val_loader, num_epochs, T, alpha,beta, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67545968",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_student(student3, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model:\n",
    "\n",
    "train_student(teacher, student4, train_loader, val_loader, num_epochs, T, alpha,beta, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ff4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_student(student4, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64902f59",
   "metadata": {},
   "source": [
    "# Final Evaluation: Playback Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data(data_path, filter_name):\n",
    "    speech_dir = os.path.join(data_path, 'speech')\n",
    "    no_speech_dir = os.path.join(data_path, 'no_speech')\n",
    "\n",
    "    speech_files = [os.path.join(speech_dir, f) for f in os.listdir(speech_dir) if f.endswith('.wav') and filter_name in f]\n",
    "    no_speech_files = [os.path.join(no_speech_dir, f) for f in os.listdir(no_speech_dir) if f.endswith('.wav') and filter_name in f]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in speech_files + no_speech_files:\n",
    "        x, sr = librosa.load(file)\n",
    "        mel_sgram = generate_mel_spectrogram(x, sr)\n",
    "        data.append(mel_sgram)\n",
    "    \n",
    "    labels = [1] * len(speech_files) + [0] * len(no_speech_files)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# Define file name filters\n",
    "file_name_filters = [\n",
    "    \"Forest_1m\",\n",
    "    \"Forest_5m\",\n",
    "    \"Forest_10m\",\n",
    "    \"Forest_20m\",\n",
    "    \"OL_1m\",\n",
    "    \"OL_5m\",\n",
    "    \"OL_10m\",\n",
    "    \"OL_20m\",\n",
    "]\n",
    "\n",
    "# Create datasets and data loaders for each filter\n",
    "datasets = {}\n",
    "data_loaders = {}\n",
    "\n",
    "for filter_name in file_name_filters:\n",
    "    data, labels = load_data(Evaluation_Data, filter_name)\n",
    "    \n",
    "    # Split data \n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.01, random_state=42)\n",
    "\n",
    "    # SpeechDataset instance\n",
    "    train_dataset = SpeechDataset(train_data, train_labels, transform=ToTensor())\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "    datasets[filter_name] = {\n",
    "        \"train\": train_dataset}\n",
    "    \n",
    "    data_loaders[filter_name] = {\n",
    "        \"train\": train_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_student(student, test_loader, device):\n",
    "    student.to(device)\n",
    "    student.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "            student_prob = torch.sigmoid(student_logits)\n",
    "\n",
    "            # Calculate AUC\n",
    "            auc = roc_auc_score(labels.cpu().numpy(), student_prob.cpu().numpy())\n",
    "            auc_scores.append(auc)\n",
    "\n",
    "            # Calculate F1\n",
    "            predicted = (student_prob > 0.5).squeeze().long().cpu().numpy()\n",
    "            f1 = f1_score(labels.cpu().numpy(), predicted)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            test_loss += criterion(student_logits, labels.float()).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    print(f'Test Loss: {test_loss}, AUC: {mean_auc}, F1 Score: {mean_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c12bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final-Eval data- Student 1\n",
    "for filter_name in file_name_filters:\n",
    "    print(f\"Evaluating {filter_name} test set\")\n",
    "    test_loader = data_loaders[filter_name][\"train\"]\n",
    "    test_student(student1, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final-Eval data- Student 2\n",
    "for filter_name in file_name_filters:\n",
    "    print(f\"Evaluating {filter_name} test set\")\n",
    "    test_loader = data_loaders[filter_name][\"train\"]\n",
    "    test_student(student2, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final-Eval data- Student 3\n",
    "for filter_name in file_name_filters:\n",
    "    print(f\"Evaluating {filter_name} test set\")\n",
    "    test_loader = data_loaders[filter_name][\"train\"]\n",
    "    test_student(student3, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final-Eval data- Student 4\n",
    "for filter_name in file_name_filters:\n",
    "    print(f\"Evaluating {filter_name} test set\")\n",
    "    test_loader = data_loaders[filter_name][\"train\"]\n",
    "    test_student(student4, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
