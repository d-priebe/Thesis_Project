{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41242ec5",
   "metadata": {},
   "source": [
    "# Teacher Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0333bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teacher Architecture: Code from Cretois et al. (2022)\n",
    "class VGG11(nn.Module):\n",
    "    def __init__(self, T=5.0):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "\n",
    "        # First set of conv layers -> depth of 64\n",
    "        self.conv11 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn11  = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Second set of conv layers -> from depth 64 to depth 128\n",
    "        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn21  = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Third set of conv layers -> from depth 128 to depth 256\n",
    "        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn31  = nn.BatchNorm2d(256)\n",
    "        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32  = nn.BatchNorm2d(256)\n",
    "                      \n",
    "        # Fourth set of conv layers -> from depth 128 to depth 256\n",
    "        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn41  = nn.BatchNorm2d(512)\n",
    "        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42  = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Fifth set of conv layers -> from depth 128 to depth 256\n",
    "        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn51  = nn.BatchNorm2d(512)\n",
    "        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn52  = nn.BatchNorm2d(512)\n",
    "              \n",
    "        # First FC layer\n",
    "        self.fc1 = nn.Linear(4 * 4 * 512,  4096)\n",
    "        # Second FC layer\n",
    "        self.fc2 = nn.Linear( 4096,  4096)\n",
    "        \n",
    "        # Add a dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Output\n",
    "        self.fc3 = nn.Linear(4096, 1)\n",
    "      \n",
    "\n",
    "    def forward(self, x,get_features = False):\n",
    "\n",
    "        # MaxPool for the first block --> img from 128x128 to 64x64\n",
    "        out = F.max_pool2d(torch.relu(self.bn11(self.conv11(x))), 2)\n",
    "\n",
    "        # MaxPool for the first block --> img from 64x64 to 32x32\n",
    "        out = F.max_pool2d(torch.relu(self.bn21(self.conv21(out))), 2)\n",
    "\n",
    "        # MaxPool for the first block --> img from 32x32 to 16x16\n",
    "        out = torch.relu(self.bn31(self.conv31(out)))\n",
    "        out = F.max_pool2d(torch.relu(self.bn32(self.conv32(out))), 2)\n",
    "        \n",
    "        # MaxPool for the first block --> img from 16x16 to 8x8\n",
    "        out = torch.relu(self.bn41(self.conv41(out)))\n",
    "        out = F.max_pool2d(torch.relu(self.bn42(self.conv42(out))), 2)\n",
    "        \n",
    "        # MaxPool for the first block --> img from 8x8 to 4x4\n",
    "        out = torch.relu(self.bn51(self.conv51(out)))\n",
    "        out = F.max_pool2d(torch.relu(self.bn52(self.conv52(out))), 2)\n",
    "        \n",
    "        # Flatten the whole thing: image of 4 x 4 * 512 \n",
    "        out = out.view(-1, 4 * 4 * 512)\n",
    "        out = self.dropout(torch.relu(self.fc1(out)))\n",
    "        out = self.dropout(torch.relu(self.fc2(out)))\n",
    "        \n",
    "        # Return:\n",
    "        logits = self.fc3(out)\n",
    "        return logits / self.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ccb29",
   "metadata": {},
   "source": [
    "# Student Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44491bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efficient Student 1\n",
    "\n",
    "# SE Layer: Code adapted from:\n",
    "#d-li14. (2021). Mobilenetv3.pytorch [Source code]. GitHub. https://github.com/d-li14/mobilenetv3.pytorch/tree/master\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        #Squeeze Operation - Generates vector size of M\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        #Excitation Operation\n",
    "        self.fc = nn.Sequential(\n",
    "            #reduces the dimensionality of the squeezed vector\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #restores the original dimensionality of the squeezed vector \n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            #Apply sigmoid to produce channel-wise scaling factors\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        #Multiply input tensor by channel- wise factor for recalibration of channel wise features\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    " # Bottleneck Layer: partial-Code adapted from:\n",
    "#d-li14. (2021). Mobilenetv3.pytorch. GitHub. https://github.com/d-li14/mobilenetv3.\n",
    "#Generative language model (GPT 3.5 OpenAI (2023))\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expansion, use_se):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        mid_channels = in_channels * expansion\n",
    "        self.use_se = use_se\n",
    "        self.layers = nn.Sequential(\n",
    "            #Pointwise - Expansion\n",
    "            nn.Conv2d(in_channels, mid_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #Depth-wise Convolution\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size, stride, padding=kernel_size // 2, groups=mid_channels, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #Pointwise - Reduction to desired output\n",
    "            nn.Conv2d(mid_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        if self.use_se:\n",
    "            self.se = SELayer(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        # Apply IRB output to SE block\n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "        out += self.shortcut(x) if self.shortcut else out\n",
    "        return nn.ReLU(inplace=True)(out)\n",
    "\n",
    "#STUDENT\n",
    "class EfficientStudent1(nn.Module):\n",
    "    def __init__(self, num_classes=1, T=5.0):\n",
    "        super(EfficientStudent1, self).__init__()\n",
    "        self.T = T\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 1, 1, bias=False), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(16, 16, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(16, 32, 3, 2, 2, use_se=False),\n",
    "            Bottleneck(32, 32, 3, 1, 4, use_se=False),\n",
    "            Bottleneck(32, 64, 3, 2, 6, use_se=True),\n",
    "            Bottleneck(64, 64, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(64, 64, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(64, 128, 3, 2, 4, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 6, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(128, 256, 3, 2, 4, use_se=True),  \n",
    "            Bottleneck(256, 256, 3, 1, 6, use_se=True),\n",
    "            Bottleneck(256, 512, 3, 2, 2, use_se=True),  \n",
    "            nn.Conv2d(512, 512, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 512, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, num_classes, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x,get_features=False):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e36541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientStudent2\n",
    "class EfficientStudent2(nn.Module):\n",
    "    def __init__(self, num_classes=1, T=5.0):\n",
    "        super(EfficientStudent2, self).__init__()\n",
    "        self.T = T\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 1, 1, bias=False), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(16, 16, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(16, 32, 3, 2, 2, use_se=False),\n",
    "            Bottleneck(32, 32, 3, 1, 4, use_se=False),\n",
    "            Bottleneck(32, 64, 3, 2, 6, use_se=True),\n",
    "            Bottleneck(64, 64, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(64, 128, 3, 2, 4, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 6, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(128, 256, 3, 2, 4, use_se=True),  \n",
    "            Bottleneck(256, 256, 3, 1, 6, use_se=True),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1, bias=False),  \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 512, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, num_classes, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientStudent3\n",
    "\n",
    "\n",
    "class EfficientStudent3(nn.Module):\n",
    "    def __init__(self, num_classes=1, T=5.0):\n",
    "        super(EfficientStudent3, self).__init__()\n",
    "        self.T = T\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, 3, 1, 1, bias=False), \n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(4, 4, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(4, 8, 3, 2, 2, use_se=False),\n",
    "            Bottleneck(8, 16, 3, 2, 4, use_se=False),\n",
    "            Bottleneck(16, 16, 3, 1, 6, use_se=True),\n",
    "            Bottleneck(16, 32, 3, 2, 2, use_se=True),\n",
    "            Bottleneck(32, 32, 3, 1, 4, use_se=True),\n",
    "            Bottleneck(32, 64, 3, 2, 6, use_se=True),\n",
    "            Bottleneck(64, 64, 3, 1, 2, use_se=True),\n",
    "            Bottleneck(64, 128, 3, 2, 4, use_se=True),\n",
    "            Bottleneck(128, 128, 3, 1, 6, use_se=True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(128, 128, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, num_classes, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead12606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientStudent4\n",
    "class EfficientStudent4(nn.Module):\n",
    "    def __init__(self, num_classes=1, T=5.0):\n",
    "        super(EfficientStudent4, self).__init__()\n",
    "        self.T = T\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Bottleneck(4, 4, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(4, 8, 3, 2, 1, use_se=False),\n",
    "            Bottleneck(8, 16, 3, 2, 1, use_se=True),\n",
    "            Bottleneck(16, 32, 3, 2, 1, use_se=True),\n",
    "            Bottleneck(32, 32, 3, 1, 1, use_se=True),\n",
    "            Bottleneck(32, 64, 3, 2, 1, use_se=True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(64, 64, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_classes, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760bdc7",
   "metadata": {},
   "source": [
    "# Feature Extraction, Dataloader, & DataSet functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mel-Spec Generation\n",
    "def generate_mel_spectrogram(x, sr, show=False, resize=True):\n",
    "    sgram = librosa.stft(x, n_fft=1024, hop_length=376)\n",
    "    sgram_mag, _ = librosa.magphase(sgram)\n",
    "    mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sr, n_mels=128)\n",
    "    mel_sgram = librosa.amplitude_to_db(mel_scale_sgram)\n",
    "    if resize:\n",
    "        # Crop the mel spectrogram to 128x128\n",
    "        mel_sgram = mel_sgram[:, :128]\n",
    "    if show:\n",
    "        librosa.display.specshow(mel_sgram, sr=sr, x_axis='time', y_axis='mel')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "    return mel_sgram\n",
    "\n",
    "# Dataset\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        x = torch.tensor(x)\n",
    "        return x, torch.tensor(y).unsqueeze(-1)\n",
    "        \n",
    "    \n",
    "  \n",
    " # Load data\n",
    "def load_data(data_path):\n",
    "    speech_dir = os.path.join(data_path, 'speech')\n",
    "    no_speech_dir = os.path.join(data_path, 'no_speech')\n",
    "\n",
    "    speech_files = [os.path.join(speech_dir, f) for f in os.listdir(speech_dir) if f.endswith('.wav')]\n",
    "    no_speech_files = [os.path.join(no_speech_dir, f) for f in os.listdir(no_speech_dir) if f.endswith('.wav')]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in speech_files + no_speech_files:\n",
    "        x, sr = librosa.load(file)\n",
    "        mel_sgram = generate_mel_spectrogram(x, sr)\n",
    "        data.append(mel_sgram)\n",
    "    \n",
    "    labels = [1] * len(speech_files) + [0] * len(no_speech_files)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d299c",
   "metadata": {},
   "source": [
    "# Training Instantiations - Models, Datasets, Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Instantiation:\n",
    "\n",
    "\n",
    "Teacher_Weights = '/content/drive/MyDrive/Thesis_Material/ecoVAD_model_weight.pt'\n",
    "Train_Data ='/content/drive/MyDrive/Thesis_Material/Synthetic_Dataset'\n",
    "\n",
    "# Load teacher model and weights\n",
    "teacher = VGG11()\n",
    "teacher.load_state_dict(torch.load(Teacher_Weights))  \n",
    "\n",
    "# student model\n",
    "student1 = EfficientStudent1()\n",
    "student2 = EfficientStudent2()\n",
    "student3 = EfficientStudent3()\n",
    "student4 = EfficientStudent4()\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs = 50\n",
    "T = 5.0\n",
    "alpha = 0.2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Load data\n",
    "data, labels = load_data(Train_Data)  \n",
    "\n",
    "# Split data into train, val, test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# SpeechDataset instances for training, validation, testing\n",
    "train_dataset = SpeechDataset(train_data, train_labels, transform=ToTensor())\n",
    "val_dataset = SpeechDataset(val_data, val_labels, transform=ToTensor())\n",
    "test_dataset = SpeechDataset(test_data, test_labels, transform=ToTensor())\n",
    "\n",
    "# DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adea8e8",
   "metadata": {},
   "source": [
    "# Define Hint & Guide Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ea462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_layers(models, layer_names):\n",
    "    extracted_layers = {}\n",
    "    \n",
    "    for model, layer_name in zip(models, layer_names):\n",
    "        for name, layer in model.named_modules():\n",
    "            if name == layer_name:\n",
    "                extracted_layers[layer_name] = layer\n",
    "                break\n",
    "                \n",
    "    return extracted_layers\n",
    "\n",
    "# List of models and their corresponding layer names\n",
    "models = [student1, teacher, student2, teacher, student3, teacher, student4, teacher]\n",
    "layer_names = [\"layers.11.layers.3\", \"conv32\", \"layers.10.layers.3\", \"conv32\", \"layers.10.layers.3\", \"conv31\", \"layers.8.layers.6\", \"conv21\"]\n",
    "\n",
    "# Extract feature layers\n",
    "extracted_layers = extract_feature_layers(models, layer_names)\n",
    "\n",
    "# Access the layers\n",
    "student_layer1 = extracted_layers[\"layers.11.layers.3\"]\n",
    "teacher_layer1 = extracted_layers[\"conv32\"]\n",
    "\n",
    "student_layer2 = extracted_layers[\"layers.10.layers.3\"]\n",
    "teacher_layer2 = extracted_layers[\"conv32\"]\n",
    "\n",
    "student_layer3 = extracted_layers[\"layers.10.layers.3\"]\n",
    "teacher_layer3 = extracted_layers[\"conv31\"]\n",
    "\n",
    "student_layer4 = extracted_layers[\"layers.8.layers.6\"]\n",
    "teacher_layer4 = extracted_layers[\"conv21\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c8de4",
   "metadata": {},
   "source": [
    "# Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Block\n",
    "def teacher_hook(module, input, output):\n",
    "    global teacher_features\n",
    "    teacher_features = output\n",
    "\n",
    "def student_hook(module, input, output):\n",
    "    global student_features\n",
    "    student_features = output\n",
    "\n",
    "def train_student(teacher, student, train_loader, val_loader, num_epochs, T, alpha, device,teacher_layer,student_layer,patience=3):\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    \n",
    "    # Freeze teacher model\n",
    "    for param in teacher.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Register forward hooks\n",
    "    teacher_layer = teacher_layer\n",
    "    student_layer =  student_layer\n",
    "\n",
    "    teacher_layer.register_forward_hook(teacher_hook)\n",
    "    student_layer.register_forward_hook(student_hook)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=.001)\n",
    "\n",
    "    # Early stopping setup\n",
    "    best_val_loss = np.inf\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "\n",
    "            # Verify the input data for both teacher and student models\n",
    "            assert inputs is not None and labels is not None, \"Input data is None\"\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through student \n",
    "            student_logits = student(inputs)\n",
    "            loss_student = criterion(student_logits, labels.float())\n",
    "\n",
    "            # Forward pass through teacher \n",
    "            teacher_logits = teacher(inputs)\n",
    "\n",
    "            assert teacher_features is not None, \"teacher_features is None\"\n",
    "            assert student_features is not None, \"student_features is None\"\n",
    "\n",
    "            # Define a 1x1 convolution layer to match the channel dimensions\n",
    "            conv1x1 = nn.Conv2d(student_features.size(1), teacher_features.size(1), kernel_size=1, stride=1, padding=0).to(device)\n",
    "\n",
    "            for param in conv1x1.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "            # Upsample student_features to match teacher_features dimensions\n",
    "            upsampled_student_features = F.interpolate(student_features, size=teacher_features.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Apply the 1x1 convolution layer to the upsampled student features\n",
    "            matched_student_features = conv1x1(upsampled_student_features)\n",
    "\n",
    "            # Calculate distillation loss using matched student features and teacher features\n",
    "            loss_distill = nn.MSELoss()(matched_student_features, teacher_features.detach())\n",
    "          \n",
    "            # Combine total loss\n",
    "            loss = loss_student + alpha * loss_distill\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / (i + 1)}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        student.eval()\n",
    "        val_loss = 0\n",
    "        auc_scores = []\n",
    "        f1_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "\n",
    "                student_logits = student(inputs)\n",
    "                student_prob = torch.sigmoid(student_logits / T)\n",
    "\n",
    "                #Calculate AUC\n",
    "                auc = roc_auc_score(labels.cpu().numpy(), student_prob.cpu().numpy())\n",
    "                auc_scores.append(auc)\n",
    "\n",
    "                #Calculate F1\n",
    "                predicted = (student_prob > 0.5).squeeze().long().cpu().numpy()\n",
    "                f1 = f1_score(labels.cpu().numpy(), predicted)\n",
    "                f1_scores.append(f1)\n",
    "            \n",
    "                val_loss += criterion(student_logits, labels.float()).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        mean_auc = np.mean(auc_scores)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        print(f'Validation Loss: {val_loss}, AUC: {mean_auc}, F1 Score: {mean_f1}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f'Early stopping after {epoch + 1} epochs')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257a0ae",
   "metadata": {},
   "source": [
    "# Test Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_student(student, test_loader, device):\n",
    "    student.eval()\n",
    "    student.to(device)\n",
    "    \n",
    "    test_loss = 0\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "            student_prob = torch.sigmoid(student_logits)\n",
    "\n",
    "            # Calculate AUC\n",
    "            auc = roc_auc_score(labels.cpu().numpy(), student_prob.cpu().numpy())\n",
    "            auc_scores.append(auc)\n",
    "\n",
    "            # Calculate F1\n",
    "            predicted = (student_prob > 0.5).squeeze().long().cpu().numpy()\n",
    "            f1 = f1_score(labels.cpu().numpy(), predicted)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            test_loss += criterion(student_logits, labels.float()).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    return test_loss, mean_auc, mean_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea671bb",
   "metadata": {},
   "source": [
    "# Train & Test Student Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4cf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train student Model 1:\n",
    "train_student(teacher, student1, train_loader, val_loader, num_epochs, T, alpha, device,teacher_layer1,student_layer1)\n",
    "\n",
    "#Test student Model 1:\n",
    "test_loss1, test_auc1, test_f11 = test_student(student1, test_loader, device)\n",
    "print(f'Student 1: Test Loss: {test_loss1}, AUC: {test_auc1}, F1 Score: {test_f11}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train student Model 2:\n",
    "train_student(teacher, student2, train_loader, val_loader, num_epochs, T, alpha, device,teacher_layer2,student_layer2)\n",
    "test_loss2, test_auc2, test_f12 = test_student(student2, test_loader, device)\n",
    "print(f'Student 2: Test Loss: {test_loss2}, AUC: {test_auc2}, F1 Score: {test_f12}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175799a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train student Model 3:\n",
    "\n",
    "train_student(teacher, student3, train_loader, val_loader, num_epochs, T, alpha, device,teacher_layer3,student_layer3)\n",
    "test_loss3, test_auc3, test_f13 = test_student(student3, test_loader, device)\n",
    "print(f'Student 3: Test Loss: {test_loss3}, AUC: {test_auc3}, F1 Score: {test_f13}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train student Model 4:\n",
    "\n",
    "train_student(teacher, student4, train_loader, val_loader, num_epochs, T, alpha, device,teacher_layer4,student_layer4)\n",
    "test_loss4, test_auc4, test_f14 = test_student(student4, test_loader, device)\n",
    "print(f'Student 4: Test Loss: {test_loss4}, AUC: {test_auc4}, F1 Score: {test_f14}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
